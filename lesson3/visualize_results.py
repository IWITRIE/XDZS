import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼ - å¢å¼ºä¸­æ–‡æ”¯æŒ
try:
    # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
    import matplotlib.font_manager as fm
    font_dirs = ['/usr/share/fonts/', '/System/Library/Fonts/', 'C:/Windows/Fonts/']
    font_files = []
    for font_dir in font_dirs:
        try:
            font_files.extend(fm.findSystemFonts(fontpaths=font_dir, fontext='ttf'))
        except:
            continue
    
    # æŸ¥æ‰¾ä¸­æ–‡å­—ä½“
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'DejaVu Sans', 'Arial Unicode MS']
    available_font = None
    for font in chinese_fonts:
        try:
            plt.rcParams['font.sans-serif'] = [font]
            # æµ‹è¯•å­—ä½“æ˜¯å¦å¯ç”¨
            fig, ax = plt.subplots(figsize=(1,1))
            ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=12)
            plt.close(fig)
            available_font = font
            break
        except:
            continue
    
    if available_font:
        plt.rcParams['font.sans-serif'] = [available_font, 'DejaVu Sans']
        print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {available_font}")
    else:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        
except Exception as e:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print(f"âš ï¸  å­—ä½“è®¾ç½®å¤±è´¥: {e}")

plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

class MLPResultsVisualizer:
    def __init__(self, csv_file='inference_results.csv'):
        """åˆå§‹åŒ–å¯è§†åŒ–å™¨"""
        try:
            self.df = pd.read_csv(csv_file)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {csv_file}")
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {self.df.shape}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„å½’ä¸€åŒ–åˆ—
            if 'Normalized_True' in self.df.columns and 'Normalized_Pred' in self.df.columns:
                print("ğŸ“ˆ æ£€æµ‹åˆ°å½’ä¸€åŒ–æ•°æ®åˆ—ï¼Œå°†è¿›è¡Œè¯¦ç»†åˆ†æ")
                self.has_normalized_data = True
            else:
                print("ğŸ“ˆ ä½¿ç”¨æ ‡å‡†æ•°æ®åˆ—è¿›è¡Œåˆ†æ")
                self.has_normalized_data = False
                
            self.setup_metrics()
            
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file}")
            print("è¯·ç¡®ä¿è¿è¡Œäº†MLPæ¨ç†ç¨‹åºå¹¶ç”Ÿæˆäº†ç»“æœæ–‡ä»¶")
            raise
        except Exception as e:
            print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise
        
    def setup_metrics(self):
        """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
        self.mse = mean_squared_error(self.df['True_Value'], self.df['Predicted_Value'])
        self.rmse = np.sqrt(self.mse)
        self.mae = mean_absolute_error(self.df['True_Value'], self.df['Predicted_Value'])
        self.r2 = r2_score(self.df['True_Value'], self.df['Predicted_Value'])
        
        # å¤„ç†MAPEè®¡ç®—ä¸­çš„é™¤é›¶é—®é¢˜
        non_zero_mask = self.df['True_Value'] != 0
        if non_zero_mask.sum() > 0:
            self.mape = np.mean(np.abs(self.df.loc[non_zero_mask, 'Error'] / self.df.loc[non_zero_mask, 'True_Value'])) * 100
        else:
            self.mape = float('inf')
        
        # æ·»åŠ ç›¸å¯¹è¯¯å·®
        self.df['Relative_Error'] = np.where(
            self.df['True_Value'] != 0,
            (self.df['Error'] / self.df['True_Value']) * 100,
            0
        )
        
        # æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        print(f"""
        ğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯:
           â€¢ æ ·æœ¬æ•°é‡: {len(self.df)}
           â€¢ çœŸå®å€¼èŒƒå›´: [{self.df['True_Value'].min():.4f}, {self.df['True_Value'].max():.4f}]
           â€¢ é¢„æµ‹å€¼èŒƒå›´: [{self.df['Predicted_Value'].min():.4f}, {self.df['Predicted_Value'].max():.4f}]
           â€¢ è¯¯å·®èŒƒå›´: [{self.df['Error'].min():.4f}, {self.df['Error'].max():.4f}]
        """)
        
    def create_comprehensive_dashboard(self):
        """åˆ›å»ºç»¼åˆä»ªè¡¨æ¿"""
        # åˆ›å»ºå­å›¾ - ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
        english_titles = (
            'True vs Predicted Values', 'Error Distribution', 'Time Series Comparison',
            'Residual Plot', 'Error Heatmap', 'Q-Q Normality Test',
            'Cumulative Error', 'Prediction Accuracy Classification', 'Performance Metrics Radar'
        )
        
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=english_titles,
            specs=[[{"type": "scatter"}, {"type": "histogram"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "heatmap"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "bar"}, {"type": "scatterpolar"}]],
            vertical_spacing=0.08, horizontal_spacing=0.08
        )
        
        # 1. çœŸå®å€¼ vs é¢„æµ‹å€¼
        fig.add_trace(
            go.Scatter(
                x=self.df['True_Value'], 
                y=self.df['Predicted_Value'],
                mode='markers',
                marker=dict(
                    size=8,
                    color=self.df['Abs_Error'],
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="Absolute Error", x=0.35)
                ),
                name='Prediction Points',
                hovertemplate='True Value: %{x:.2f}<br>Predicted Value: %{y:.2f}<br>Error: %{marker.color:.2f}<extra></extra>'
            ),
            row=1, col=1
        )
        
        # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
        min_val, max_val = min(self.df['True_Value'].min(), self.df['Predicted_Value'].min()), \
                          max(self.df['True_Value'].max(), self.df['Predicted_Value'].max())
        fig.add_trace(
            go.Scatter(x=[min_val, max_val], y=[min_val, max_val], 
                      mode='lines', name='Perfect Prediction', line=dict(color='red', dash='dash')),
            row=1, col=1
        )
        
        # 2. è¯¯å·®åˆ†å¸ƒ
        fig.add_trace(
            go.Histogram(
                x=self.df['Error'],
                nbinsx=30,
                name='Error Distribution',
                marker_color='lightblue',
                opacity=0.7
            ),
            row=1, col=2
        )
        
        # 3. æ—¶é—´åºåˆ—å¯¹æ¯”
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'], 
                y=self.df['True_Value'],
                mode='lines+markers',
                name='True Values',
                line=dict(color='blue', width=2)
            ),
            row=1, col=3
        )
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'], 
                y=self.df['Predicted_Value'],
                mode='lines+markers',
                name='Predicted Values',
                line=dict(color='red', width=2)
            ),
            row=1, col=3
        )
        
        # 4. æ®‹å·®å›¾
        fig.add_trace(
            go.Scatter(
                x=self.df['Predicted_Value'], 
                y=self.df['Error'],
                mode='markers',
                marker=dict(size=6, color='orange', opacity=0.6),
                name='Residuals'
            ),
            row=2, col=1
        )
        
        # æ·»åŠ é›¶çº¿
        fig.add_hline(y=0, line_dash="dash", line_color="red", row=2, col=1)
        
        # 5. è¯¯å·®çƒ­åŠ›å›¾ (åˆ†æ®µ)
        position_bins = pd.cut(self.df['Position'], bins=10)
        value_bins = pd.cut(self.df['True_Value'], bins=10)
        heatmap_data = self.df.groupby([position_bins, value_bins])['Abs_Error'].mean().unstack()
        
        fig.add_trace(
            go.Heatmap(
                z=heatmap_data.values,
                colorscale='RdYlBu_r',
                showscale=True,
                colorbar=dict(title="Mean Absolute Error", x=0.68)
            ),
            row=2, col=2
        )
        
        # 6. Q-Qå›¾
        theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(self.df)))
        sample_quantiles = np.sort(self.df['Error'])
        
        fig.add_trace(
            go.Scatter(
                x=theoretical_quantiles,
                y=sample_quantiles,
                mode='markers',
                marker=dict(size=6, color='purple'),
                name='Q-Q Points'
            ),
            row=2, col=3
        )
        
        # 7. ç´¯ç§¯è¯¯å·®
        cumulative_error = np.cumsum(np.abs(self.df['Error']))
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=cumulative_error,
                mode='lines',
                fill='tonexty',
                name='Cumulative Error',
                line=dict(color='green', width=3)
            ),
            row=3, col=1
        )
        
        # 8. é¢„æµ‹å‡†ç¡®æ€§åˆ†çº§
        accuracy_bins = ['Excellent (<1%)', 'Good (1-3%)', 'Fair (3-5%)', 'Poor (>5%)']
        accuracy_counts = [
            sum(self.df['Relative_Error'].abs() < 1),
            sum((self.df['Relative_Error'].abs() >= 1) & (self.df['Relative_Error'].abs() < 3)),
            sum((self.df['Relative_Error'].abs() >= 3) & (self.df['Relative_Error'].abs() < 5)),
            sum(self.df['Relative_Error'].abs() >= 5)
        ]
        
        fig.add_trace(
            go.Bar(
                x=accuracy_bins,
                y=accuracy_counts,
                marker_color=['green', 'yellow', 'orange', 'red'],
                name='Accuracy Classification'
            ),
            row=3, col=2
        )
        
        # 9. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        metrics = ['RÂ²', 'RMSE', 'MAE', 'MAPE']
        values = [self.r2, 1-min(self.rmse/max(self.df['True_Value']), 1), 
                 1-min(self.mae/max(self.df['True_Value']), 1), 1-min(self.mape/100, 1)]
        
        fig.add_trace(
            go.Scatterpolar(
                r=values,
                theta=metrics,
                fill='toself',
                name='Performance Metrics',
                line_color='navy'
            ),
            row=3, col=3
        )
        
        # æ›´æ–°å¸ƒå±€ - è‹±æ–‡æ ‡é¢˜
        fig.update_layout(
            height=1200,
            title_text=f"MLP Neural Network Prediction Results Comprehensive Analysis<br>RÂ² Coefficient={self.r2:.4f}, RMSE={self.rmse:.4f}, MAE={self.mae:.4f}",
            title_x=0.5,
            showlegend=False,
            font=dict(size=11)
        )
        
        # ä¿å­˜å’Œæ˜¾ç¤º
        fig.write_html("mlp_comprehensive_analysis.html")
        print("ğŸ“„ äº¤äº’å¼ç»¼åˆåˆ†æå·²ä¿å­˜è‡³: mlp_comprehensive_analysis.html")
        
        # ä¸­æ–‡æ€§èƒ½æŠ¥å‘Š
        accuracy_counts = [
            sum(self.df['Relative_Error'].abs() < 1),
            sum((self.df['Relative_Error'].abs() >= 1) & (self.df['Relative_Error'].abs() < 3)),
            sum((self.df['Relative_Error'].abs() >= 3) & (self.df['Relative_Error'].abs() < 5)),
            sum(self.df['Relative_Error'].abs() >= 5)
        ]
        
        print(f"""
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    æ¨¡å‹æ€§èƒ½è¯¦ç»†æŠ¥å‘Š
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ğŸ“Š æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡:
           â€¢ RÂ² å†³å®šç³»æ•°: {self.r2:.6f} {'(ä¼˜ç§€)' if self.r2 > 0.9 else '(è‰¯å¥½)' if self.r2 > 0.8 else '(ä¸€èˆ¬)' if self.r2 > 0.7 else '(éœ€è¦æ”¹è¿›)'}
           â€¢ RMSE å‡æ–¹æ ¹è¯¯å·®: {self.rmse:.6f}
           â€¢ MAE å¹³å‡ç»å¯¹è¯¯å·®: {self.mae:.6f}
           â€¢ MAPE å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®: {self.mape:.4f}%
        
        ğŸ“ˆ é¢„æµ‹å‡†ç¡®æ€§åˆ†æ:
           â€¢ ğŸŸ¢ ä¼˜ç§€é¢„æµ‹ (ç›¸å¯¹è¯¯å·® < 1%): {accuracy_counts[0]} ä¸ªæ ·æœ¬ ({accuracy_counts[0]/len(self.df)*100:.1f}%)
           â€¢ ğŸŸ¡ è‰¯å¥½é¢„æµ‹ (ç›¸å¯¹è¯¯å·® 1-3%): {accuracy_counts[1]} ä¸ªæ ·æœ¬ ({accuracy_counts[1]/len(self.df)*100:.1f}%)
           â€¢ ğŸŸ  ä¸€èˆ¬é¢„æµ‹ (ç›¸å¯¹è¯¯å·® 3-5%): {accuracy_counts[2]} ä¸ªæ ·æœ¬ ({accuracy_counts[2]/len(self.df)*100:.1f}%)
           â€¢ ğŸ”´ è¾ƒå·®é¢„æµ‹ (ç›¸å¯¹è¯¯å·® > 5%): {accuracy_counts[3]} ä¸ªæ ·æœ¬ ({accuracy_counts[3]/len(self.df)*100:.1f}%)
        
        ğŸ¯ æ¨¡å‹æ•´ä½“è¯„ä¼°:
           é¢„æµ‹è´¨é‡ç­‰çº§: {'ğŸ† ä¼˜ç§€' if self.r2 > 0.9 else 'âœ… è‰¯å¥½' if self.r2 > 0.8 else 'âš ï¸  ä¸€èˆ¬' if self.r2 > 0.7 else 'âŒ éœ€è¦æ”¹è¿›'}
           
        {'ğŸ” è°ƒè¯•ä¿¡æ¯:' if self.has_normalized_data else ''}
        {f'   â€¢ æ£€æµ‹åˆ°å½’ä¸€åŒ–æ•°æ®ï¼Œå¯èƒ½å­˜åœ¨æ ‡å‡†åŒ–é—®é¢˜' if self.has_normalized_data and abs(self.df["Predicted_Value"].mean() - self.df["True_Value"].mean()) > self.df["True_Value"].std() else ''}
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # å¦‚æœæœ‰å½’ä¸€åŒ–æ•°æ®ï¼Œè¿›è¡Œé¢å¤–åˆ†æ
        if self.has_normalized_data:
            self.analyze_normalization_issues()
    
    def analyze_normalization_issues(self):
        """åˆ†æå½’ä¸€åŒ–ç›¸å…³é—®é¢˜"""
        print("\nğŸ”¬ å½’ä¸€åŒ–æ•°æ®åˆ†æ:")
        print(f"   â€¢ å½’ä¸€åŒ–çœŸå®å€¼èŒƒå›´: [{self.df['Normalized_True'].min():.4f}, {self.df['Normalized_True'].max():.4f}]")
        print(f"   â€¢ å½’ä¸€åŒ–é¢„æµ‹å€¼èŒƒå›´: [{self.df['Normalized_Pred'].min():.4f}, {self.df['Normalized_Pred'].max():.4f}]")
        
        # æ£€æŸ¥å½’ä¸€åŒ–é¢„æµ‹å€¼æ˜¯å¦åˆç†
        norm_pred_mean = self.df['Normalized_Pred'].mean()
        norm_true_mean = self.df['Normalized_True'].mean()
        
        if abs(norm_pred_mean - 0.5) > 0.3:
            print(f"   âš ï¸  è­¦å‘Š: å½’ä¸€åŒ–é¢„æµ‹å€¼å‡å€¼ ({norm_pred_mean:.4f}) åç¦»æœŸæœ›å€¼ (0.5)")
        
        if abs(norm_pred_mean - norm_true_mean) > 0.2:
            print(f"   âš ï¸  è­¦å‘Š: å½’ä¸€åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼å‡å€¼å·®å¼‚è¾ƒå¤§ ({abs(norm_pred_mean - norm_true_mean):.4f})")
            print(f"      è¿™å¯èƒ½è¡¨æ˜æ¨¡å‹è®­ç»ƒæˆ–å½’ä¸€åŒ–è¿‡ç¨‹å­˜åœ¨é—®é¢˜")

    def create_statistical_analysis(self):
        """åˆ›å»ºç»Ÿè®¡åˆ†æå›¾è¡¨"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('MLP Model Statistical Analysis Report', fontsize=16, fontweight='bold')
        
        # 1. è¯¯å·®åˆ†å¸ƒä¸æ­£æ€æ€§æ£€éªŒ
        axes[0, 0].hist(self.df['Error'], bins=30, alpha=0.7, color='skyblue', density=True)
        mu, sigma = stats.norm.fit(self.df['Error'])
        x = np.linspace(self.df['Error'].min(), self.df['Error'].max(), 100)
        axes[0, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, 
                       label=f'Normal Fit Î¼={mu:.3f}, Ïƒ={sigma:.3f}')
        axes[0, 0].set_title('Error Distribution and Normality Test')
        axes[0, 0].set_xlabel('Prediction Error')
        axes[0, 0].set_ylabel('Probability Density')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ç›¸å…³æ€§åˆ†æ
        correlation = np.corrcoef(self.df['True_Value'], self.df['Predicted_Value'])[0, 1]
        axes[0, 1].scatter(self.df['True_Value'], self.df['Predicted_Value'], alpha=0.6, s=50)
        z = np.polyfit(self.df['True_Value'], self.df['Predicted_Value'], 1)
        p = np.poly1d(z)
        axes[0, 1].plot(self.df['True_Value'], p(self.df['True_Value']), "r--", alpha=0.8, lw=2)
        axes[0, 1].set_xlabel('True Values')
        axes[0, 1].set_ylabel('Predicted Values')
        axes[0, 1].set_title(f'True vs Predicted Values Correlation (r={correlation:.4f})')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®éšæ—¶é—´å˜åŒ–
        axes[0, 2].plot(self.df['Position'], self.df['Error'], 'b-', alpha=0.7, linewidth=1)
        axes[0, 2].axhline(y=0, color='r', linestyle='--', alpha=0.8)
        axes[0, 2].fill_between(self.df['Position'], self.df['Error'], alpha=0.3)
        axes[0, 2].set_title('Residual Time Series')
        axes[0, 2].set_xlabel('Position')
        axes[0, 2].set_ylabel('Error')
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. é¢„æµ‹åŒºé—´åˆ†æ
        percentiles = [10, 25, 50, 75, 90]
        error_percentiles = [np.percentile(self.df['Abs_Error'], p) for p in percentiles]
        axes[1, 0].bar([f'{p}%' for p in percentiles], error_percentiles, 
                      color=['lightgreen', 'yellow', 'orange', 'red', 'darkred'], alpha=0.7)
        axes[1, 0].set_title('Absolute Error Percentiles')
        axes[1, 0].set_ylabel('Absolute Error')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. æ»‘åŠ¨çª—å£è¯¯å·®
        window_size = max(10, len(self.df) // 20)
        rolling_mae = pd.Series(self.df['Abs_Error']).rolling(window=window_size).mean()
        axes[1, 1].plot(self.df['Position'], rolling_mae, 'g-', linewidth=2, label=f'Rolling MAE (window={window_size})')
        axes[1, 1].fill_between(self.df['Position'], rolling_mae, alpha=0.3, color='green')
        axes[1, 1].set_title('Rolling Window Mean Absolute Error')
        axes[1, 1].set_xlabel('Position')
        axes[1, 1].set_ylabel('MAE')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. è¯¯å·®ç®±çº¿å›¾
        error_ranges = pd.cut(self.df['True_Value'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        error_by_range = [self.df[error_ranges == label]['Abs_Error'].values for label in error_ranges.cat.categories]
        bp = axes[1, 2].boxplot(error_by_range, labels=error_ranges.cat.categories, patch_artist=True)
        colors = ['lightblue', 'lightgreen', 'yellow', 'orange', 'red']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
        axes[1, 2].set_title('Error Distribution by True Value Range')
        axes[1, 2].set_xlabel('True Value Range')
        axes[1, 2].set_ylabel('Absolute Error')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('mlp_statistical_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“„ ç»Ÿè®¡åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: mlp_statistical_analysis.png")
        plt.show()
    
    def create_3d_analysis(self):
        """åˆ›å»º3Dåˆ†æå›¾"""
        fig = go.Figure()
        
        # 3Dæ•£ç‚¹å›¾
        fig.add_trace(go.Scatter3d(
            x=self.df['Position'],
            y=self.df['True_Value'],
            z=self.df['Predicted_Value'],
            mode='markers',
            marker=dict(
                size=8,
                color=self.df['Abs_Error'],
                colorscale='Rainbow',
                showscale=True,
                colorbar=dict(title="Absolute Error")
            ),
            text=[f'Position: {pos}<br>True: {true:.2f}<br>Predicted: {pred:.2f}<br>Error: {err:.2f}' 
                  for pos, true, pred, err in zip(self.df['Position'], self.df['True_Value'], 
                                                 self.df['Predicted_Value'], self.df['Error'])],
            hovertemplate='%{text}<extra></extra>',
            name='Prediction Results'
        ))
        
        fig.update_layout(
            title='MLP Prediction Results 3D Visualization Analysis',
            scene=dict(
                xaxis_title='Time Position',
                yaxis_title='True Values',
                zaxis_title='Predicted Values',
                camera=dict(eye=dict(x=1.2, y=1.2, z=1.2))
            ),
            height=600
        )
        
        fig.write_html("mlp_3d_analysis.html")
        print("ğŸ“„ 3Då¯è§†åŒ–åˆ†æå·²ä¿å­˜è‡³: mlp_3d_analysis.html")
    
    def create_model_diagnosis(self):
        """åˆ›å»ºæ¨¡å‹è¯Šæ–­åˆ†æ"""
        print("\nğŸ” æ·±åº¦æ¨¡å‹è¯Šæ–­åˆ†æ:")
        
        # åˆ†æé¢„æµ‹å€¼çš„åˆ†å¸ƒé—®é¢˜
        pred_range = self.df['Predicted_Value'].max() - self.df['Predicted_Value'].min()
        true_range = self.df['True_Value'].max() - self.df['True_Value'].min()
        
        print(f"   ğŸ“ˆ é¢„æµ‹èŒƒå›´é—®é¢˜:")
        print(f"      â€¢ çœŸå®å€¼èŒƒå›´: {true_range:.4f}")
        print(f"      â€¢ é¢„æµ‹å€¼èŒƒå›´: {pred_range:.4f}")
        print(f"      â€¢ èŒƒå›´æ¯”ä¾‹: {pred_range/true_range:.4f}")
        
        if pred_range/true_range < 0.5:
            print(f"      âš ï¸  ä¸¥é‡é—®é¢˜: é¢„æµ‹å€¼èŒƒå›´è¿‡çª„ï¼Œæ¨¡å‹å­¦ä¹ ä¸å……åˆ†")
            print(f"      ğŸ’¡ å»ºè®®: å¢åŠ è®­ç»ƒè½®æ•°ã€è°ƒæ•´å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
        
        # åˆ†æåç§»é—®é¢˜
        pred_mean = self.df['Predicted_Value'].mean()
        true_mean = self.df['True_Value'].mean()
        bias = pred_mean - true_mean
        
        print(f"\n   ğŸ“Š é¢„æµ‹åç§»åˆ†æ:")
        print(f"      â€¢ çœŸå®å€¼å‡å€¼: {true_mean:.4f}")
        print(f"      â€¢ é¢„æµ‹å€¼å‡å€¼: {pred_mean:.4f}")
        print(f"      â€¢ ç³»ç»Ÿæ€§åç§»: {bias:.4f}")
        
        if abs(bias) > true_mean * 0.1:
            print(f"      âš ï¸  é—®é¢˜: å­˜åœ¨æ˜¾è‘—ç³»ç»Ÿæ€§åç§»")
            print(f"      ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€æƒé‡åˆå§‹åŒ–æˆ–å­¦ä¹ ç‡è®¾ç½®")
        
        # å½’ä¸€åŒ–é—®é¢˜åˆ†æ
        if self.has_normalized_data:
            norm_pred_std = self.df['Normalized_Pred'].std()
            norm_true_std = self.df['Normalized_True'].std()
            
            print(f"\n   ğŸ”¬ å½’ä¸€åŒ–ç©ºé—´åˆ†æ:")
            print(f"      â€¢ å½’ä¸€åŒ–çœŸå®å€¼æ ‡å‡†å·®: {norm_true_std:.4f}")
            print(f"      â€¢ å½’ä¸€åŒ–é¢„æµ‹å€¼æ ‡å‡†å·®: {norm_pred_std:.4f}")
            print(f"      â€¢ æ–¹å·®æ¯”ä¾‹: {norm_pred_std/norm_true_std:.4f}")
            
            if norm_pred_std/norm_true_std < 0.3:
                print(f"      âš ï¸  ä¸¥é‡é—®é¢˜: é¢„æµ‹å€¼åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸­æ–¹å·®è¿‡å°")
                print(f"      ğŸ’¡ å¯èƒ½åŸå› : æ¢¯åº¦æ¶ˆå¤±ã€å­¦ä¹ ç‡è¿‡å°ã€æˆ–æ¿€æ´»å‡½æ•°é¥±å’Œ")
                
        # æä¾›æ”¹è¿›å»ºè®®
        print(f"\n   ğŸ’¡ æ¨¡å‹æ”¹è¿›å»ºè®®:")
        if self.r2 < 0.3:
            print(f"      ğŸ”§ æ¶æ„è°ƒæ•´: è€ƒè™‘å¢åŠ éšè—å±‚ç»´åº¦æˆ–å±‚æ•°")
            print(f"      ğŸ“š æ•°æ®å¢å¼º: æ£€æŸ¥æ•°æ®è´¨é‡å’Œç‰¹å¾å·¥ç¨‹")
            print(f"      âš™ï¸  è¶…å‚æ•°: å°è¯•ä¸åŒçš„å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨")
        
        if abs(bias) > true_mean * 0.05:
            print(f"      ğŸ¯ åç§»ä¿®æ­£: è€ƒè™‘æ·»åŠ åç½®ä¿®æ­£æˆ–è°ƒæ•´æŸå¤±å‡½æ•°")
            
        if pred_range/true_range < 0.5:
            print(f"      ğŸ“ˆ æ¿€æ´»æ”¹è¿›: æ£€æŸ¥æ¿€æ´»å‡½æ•°é€‰æ‹©å’Œæƒé‡åˆå§‹åŒ–æ–¹æ¡ˆ")
    
    def create_time_series_plot(self):
        """åˆ›å»ºæ—¶é—´åºåˆ—æ›²çº¿å›¾"""
        fig = go.Figure()
        
        # æ·»åŠ çœŸå®å€¼æ›²çº¿
        fig.add_trace(go.Scatter(
            x=self.df['Position'],
            y=self.df['True_Value'],
            mode='lines+markers',
            name='True Values',
            line=dict(color='blue', width=2),
            marker=dict(size=4),
            hovertemplate='Position: %{x}<br>True Value: %{y:.2f}<extra></extra>'
        ))
        
        # æ·»åŠ é¢„æµ‹å€¼æ›²çº¿
        fig.add_trace(go.Scatter(
            x=self.df['Position'],
            y=self.df['Predicted_Value'],
            mode='lines+markers',
            name='Predicted Values',
            line=dict(color='red', width=2, dash='dash'),
            marker=dict(size=4),
            hovertemplate='Position: %{x}<br>Predicted Value: %{y:.2f}<extra></extra>'
        ))
        
        # æ·»åŠ è¯¯å·®å¡«å……åŒºåŸŸ
        fig.add_trace(go.Scatter(
            x=self.df['Position'],
            y=self.df['True_Value'] + self.df['Error'],
            fill=None,
            mode='lines',
            line_color='rgba(0,100,80,0)',
            showlegend=False
        ))
        
        fig.add_trace(go.Scatter(
            x=self.df['Position'],
            y=self.df['True_Value'] - self.df['Error'],
            fill='tonexty',
            mode='lines',
            line_color='rgba(0,100,80,0)',
            name='Error Band',
            fillcolor='rgba(255,0,0,0.1)'
        ))
        
        fig.update_layout(
            title='Time Series: True Values vs Predicted Values',
            xaxis_title='Time Position',
            yaxis_title='Values',
            hovermode='x unified',
            height=600,
            showlegend=True,
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="left",
                x=0.01
            )
        )
        
        fig.write_html("mlp_time_series.html")
        print("ğŸ“„ æ—¶é—´åºåˆ—æ›²çº¿å›¾å·²ä¿å­˜è‡³: mlp_time_series.html")

    def create_detailed_comparison_plot(self):
        """åˆ›å»ºè¯¦ç»†å¯¹æ¯”å›¾è¡¨"""
        # åˆ›å»ºå¤šå­å›¾å¸ƒå±€
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                'Time Series Comparison',
                'Prediction Error Over Time',
                'Rolling Average Comparison',
                'Prediction Confidence Analysis'
            ],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # 1. æ—¶é—´åºåˆ—å¯¹æ¯”
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=self.df['True_Value'],
                mode='lines',
                name='True Values',
                line=dict(color='blue', width=2)
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=self.df['Predicted_Value'],
                mode='lines',
                name='Predicted Values',
                line=dict(color='red', width=2, dash='dash')
            ),
            row=1, col=1
        )
        
        # 2. è¯¯å·®éšæ—¶é—´å˜åŒ–
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=self.df['Error'],
                mode='lines',
                name='Prediction Error',
                line=dict(color='orange', width=1.5),
                fill='tonexty'
            ),
            row=1, col=2
        )
        
        # æ·»åŠ é›¶çº¿
        fig.add_hline(y=0, line_dash="dash", line_color="gray", row=1, col=2)
        
        # 3. æ»‘åŠ¨å¹³å‡å¯¹æ¯”
        window_size = max(10, len(self.df) // 30)
        rolling_true = pd.Series(self.df['True_Value']).rolling(window=window_size).mean()
        rolling_pred = pd.Series(self.df['Predicted_Value']).rolling(window=window_size).mean()
        
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=rolling_true,
                mode='lines',
                name=f'True Values (MA-{window_size})',
                line=dict(color='darkblue', width=3)
            ),
            row=2, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=self.df['Position'],
                y=rolling_pred,
                mode='lines',
                name=f'Predicted Values (MA-{window_size})',
                line=dict(color='darkred', width=3)
            ),
            row=2, col=1
        )
        
        # 4. é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
        abs_errors = self.df['Abs_Error']
        confidence_levels = pd.cut(abs_errors, bins=5, labels=['Very High', 'High', 'Medium', 'Low', 'Very Low'])
        confidence_counts = confidence_levels.value_counts()
        
        fig.add_trace(
            go.Bar(
                x=confidence_counts.index,
                y=confidence_counts.values,
                name='Prediction Confidence',
                marker_color=['green', 'lightgreen', 'yellow', 'orange', 'red']
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            height=800,
            title_text="Detailed MLP Prediction Analysis",
            showlegend=True
        )
        
        fig.write_html("mlp_detailed_comparison.html")
        print("ğŸ“„ è¯¦ç»†å¯¹æ¯”åˆ†æå·²ä¿å­˜è‡³: mlp_detailed_comparison.html")

    def create_matplotlib_plots(self):
        """åˆ›å»ºmatplotlibé™æ€å›¾è¡¨"""
        # åˆ›å»ºå¤§å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(20, 12))
        fig.suptitle('MLP Neural Network Prediction Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ—¶é—´åºåˆ—å¯¹æ¯”
        axes[0, 0].plot(self.df['Position'], self.df['True_Value'], 
                       'b-', linewidth=2, label='True Values', alpha=0.8)
        axes[0, 0].plot(self.df['Position'], self.df['Predicted_Value'], 
                       'r--', linewidth=2, label='Predicted Values', alpha=0.8)
        axes[0, 0].fill_between(self.df['Position'], 
                               self.df['True_Value'], 
                               self.df['Predicted_Value'],
                               alpha=0.3, color='gray', label='Error Area')
        axes[0, 0].set_title('Time Series: True vs Predicted Values')
        axes[0, 0].set_xlabel('Time Position')
        axes[0, 0].set_ylabel('Values')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è¯¯å·®æ—¶é—´åºåˆ—
        axes[0, 1].plot(self.df['Position'], self.df['Error'], 
                       'g-', linewidth=1.5, alpha=0.7)
        axes[0, 1].fill_between(self.df['Position'], self.df['Error'], 
                               alpha=0.3, color='green')
        axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.8)
        axes[0, 1].set_title('Prediction Error Over Time')
        axes[0, 1].set_xlabel('Time Position')
        axes[0, 1].set_ylabel('Error')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ•£ç‚¹å›¾ä¸æ‹Ÿåˆçº¿
        axes[1, 0].scatter(self.df['True_Value'], self.df['Predicted_Value'], 
                          alpha=0.6, s=30, c=self.df['Abs_Error'], 
                          cmap='viridis')
        
        # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
        min_val = min(self.df['True_Value'].min(), self.df['Predicted_Value'].min())
        max_val = max(self.df['True_Value'].max(), self.df['Predicted_Value'].max())
        axes[1, 0].plot([min_val, max_val], [min_val, max_val], 
                       'r--', linewidth=2, label='Perfect Prediction')
        
        # æ·»åŠ æ‹Ÿåˆçº¿
        z = np.polyfit(self.df['True_Value'], self.df['Predicted_Value'], 1)
        p = np.poly1d(z)
        axes[1, 0].plot(self.df['True_Value'], p(self.df['True_Value']), 
                       'orange', linewidth=2, label=f'Fit Line (RÂ²={self.r2:.3f})')
        
        axes[1, 0].set_title('True vs Predicted Values (Scatter)')
        axes[1, 0].set_xlabel('True Values')
        axes[1, 0].set_ylabel('Predicted Values')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. é¢„æµ‹å‡†ç¡®åº¦åˆ†å¸ƒ
        accuracy_ranges = ['<1%', '1-3%', '3-5%', '5-10%', '>10%']
        accuracy_counts = [
            sum(self.df['Relative_Error'].abs() < 1),
            sum((self.df['Relative_Error'].abs() >= 1) & (self.df['Relative_Error'].abs() < 3)),
            sum((self.df['Relative_Error'].abs() >= 3) & (self.df['Relative_Error'].abs() < 5)),
            sum((self.df['Relative_Error'].abs() >= 5) & (self.df['Relative_Error'].abs() < 10)),
            sum(self.df['Relative_Error'].abs() >= 10)
        ]
        
        colors = ['green', 'lightgreen', 'yellow', 'orange', 'red']
        bars = axes[1, 1].bar(accuracy_ranges, accuracy_counts, color=colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, accuracy_counts):
            height = bar.get_height()
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,
                           f'{count}\n({count/len(self.df)*100:.1f}%)',
                           ha='center', va='bottom')
        
        axes[1, 1].set_title('Prediction Accuracy Distribution')
        axes[1, 1].set_xlabel('Relative Error Range')
        axes[1, 1].set_ylabel('Number of Samples')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig('mlp_comprehensive_curves.png', dpi=300, bbox_inches='tight')
        print("ğŸ“„ ç»¼åˆæ›²çº¿åˆ†æå›¾å·²ä¿å­˜è‡³: mlp_comprehensive_curves.png")
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹MLPç»“æœå¯è§†åŒ–åˆ†æ...")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = MLPResultsVisualizer()
        
        # ç”Ÿæˆç»¼åˆä»ªè¡¨æ¿
        print("\nğŸ“Š ç”Ÿæˆç»¼åˆåˆ†æä»ªè¡¨æ¿...")
        visualizer.create_comprehensive_dashboard()
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—æ›²çº¿å›¾
        print("\nğŸ“ˆ ç”Ÿæˆæ—¶é—´åºåˆ—æ›²çº¿å›¾...")
        visualizer.create_time_series_plot()
        
        # ç”Ÿæˆè¯¦ç»†å¯¹æ¯”å›¾è¡¨
        print("\nğŸ” ç”Ÿæˆè¯¦ç»†å¯¹æ¯”åˆ†æ...")
        visualizer.create_detailed_comparison_plot()
        
        # ç”Ÿæˆmatplotlibé™æ€å›¾è¡¨
        print("\nğŸ“Š ç”Ÿæˆmatplotlibé™æ€å›¾è¡¨...")
        visualizer.create_matplotlib_plots()
        
        # ç”Ÿæˆ3Dåˆ†æ
        print("\nğŸ¯ ç”Ÿæˆ3Då¯è§†åŒ–åˆ†æ...")
        visualizer.create_3d_analysis()
        
        # æ¨¡å‹è¯Šæ–­åˆ†æ
        print("\nğŸ”¬ æ‰§è¡Œæ¨¡å‹è¯Šæ–­åˆ†æ...")
        visualizer.create_model_diagnosis()
        
        # ç”Ÿæˆç»Ÿè®¡åˆ†æ
        print("\nğŸ“ˆ ç”Ÿæˆç»Ÿè®¡åˆ†æå›¾è¡¨...")
        visualizer.create_statistical_analysis()
        
        print("\n" + "=" * 50)
        print("âœ… å¯è§†åŒ–åˆ†æå®Œæˆ!")
        print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â€¢ mlp_comprehensive_analysis.html - äº¤äº’å¼ç»¼åˆåˆ†æä»ªè¡¨æ¿")
        print("   â€¢ mlp_time_series.html - æ—¶é—´åºåˆ—æ›²çº¿å›¾")
        print("   â€¢ mlp_detailed_comparison.html - è¯¦ç»†å¯¹æ¯”åˆ†æ")
        print("   â€¢ mlp_comprehensive_curves.png - ç»¼åˆæ›²çº¿åˆ†æå›¾")
        print("   â€¢ mlp_3d_analysis.html - 3Dç«‹ä½“å¯è§†åŒ–åˆ†æ")
        print("   â€¢ mlp_statistical_analysis.png - è¯¦ç»†ç»Ÿè®¡åˆ†æå›¾è¡¨")
        print("\nğŸ’¡ æç¤º: åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å›¾è¡¨")
        print("\nğŸ¯ æ ¹æ®è¯Šæ–­ç»“æœï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè®­ç»ƒå‚æ•°")
        
    except Exception as e:
        print(f"\nâŒ å¯è§†åŒ–åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")

if __name__ == "__main__":
    main()
